{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 TP: Explainability Analysis with SHAP and LIME\n",
    "\n",
    "**Course:** M2 SID - Processus Data \n",
    "**Instructor:** Feda Almuhisen \n",
    "**Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, you will:\n",
    "- Apply **SHAP** (SHapley Additive exPlanations) for global and local feature importance\n",
    "- Apply **LIME** (Local Interpretable Model-agnostic Explanations) for instance-level explanations\n",
    "- Compare SHAP and LIME results\n",
    "- Interpret ML predictions for non-technical stakeholders\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Completed Day 1 TP (trained autoencoder model) \n",
    "Installed Day 2 dependencies: `pip install shap lime` \n",
    "Copied `explainability_utils.py` to `models/` folder\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import from your Day 1 modules\n",
    "from data.data_loader import load_data\n",
    "from data.preprocessing import normalize_data, remove_constant_features\n",
    "from models.autoencoder import Autoencoder\n",
    "\n",
    "# Import Day 2 explainability utilities\n",
    "from models import explainability_utils as exp_utils\n",
    "\n",
    "# SHAP and LIME\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\" All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Data and Model from Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data (same as Day 1)\n",
    "print(\"Loading test data...\")\n",
    "test_df = load_data('data/processed/test_FD001.txt')\n",
    "\n",
    "# Preprocess (same as Day 1)\n",
    "sensor_cols = [col for col in test_df.columns if col.startswith('sensor')]\n",
    "X_test_raw = test_df[sensor_cols].values\n",
    "\n",
    "# Remove constant features\n",
    "X_test, active_sensor_indices = remove_constant_features(X_test_raw)\n",
    "feature_names = [sensor_cols[i] for i in active_sensor_indices]\n",
    "\n",
    "# Normalize\n",
    "X_test_normalized, scaler = normalize_data(X_test)\n",
    "\n",
    "print(f\"Test data shape: {X_test_normalized.shape}\")\n",
    "print(f\"Active sensors ({len(feature_names)}): {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model from Day 1\n",
    "# UPDATE THIS PATH to match your model file!\n",
    "model_path = \"../models/autoencoder_epoch_50.pth\" # CHANGE THIS!\n",
    "\n",
    "# Check available models\n",
    "import glob\n",
    "available_models = glob.glob(\"../models/*.pth\")\n",
    "print(\"Available models:\")\n",
    "for model in available_models:\n",
    "print(f\" - {model}\")\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = X_test_normalized.shape[1]\n",
    "encoding_dim = 16  # Match pretrained model architecture\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, encoding_dim=encoding_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n Model loaded from: {model_path}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Calculate Reconstruction Errors\n",
    "\n",
    "First, let's calculate reconstruction errors for all test samples (same as Day 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors\n",
    "errors = exp_utils.calculate_reconstruction_error(model, X_test_normalized, device)\n",
    "\n",
    "print(f\"Reconstruction errors calculated\")\n",
    "print(f\" Mean: {errors.mean():.4f}\")\n",
    "print(f\" Std: {errors.std():.4f}\")\n",
    "print(f\" Min: {errors.min():.4f}\")\n",
    "print(f\" Max: {errors.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies\n",
    "threshold, is_anomaly = exp_utils.detect_anomalies(errors, threshold_method=\"mean_plus_2std\")\n",
    "\n",
    "print(f\"Anomaly threshold: {threshold:.4f}\")\n",
    "print(f\"Anomalies detected: {is_anomaly.sum()} / {len(is_anomaly)} ({100*is_anomaly.sum()/len(is_anomaly):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(errors, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold:.4f}')\n",
    "plt.xlabel('Reconstruction Error', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Frequency', fontweight='bold', fontsize=12)\n",
    "plt.title('Distribution of Reconstruction Errors', fontweight='bold', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Blue region: Normal samples (error < {threshold:.4f})\")\n",
    "print(f\"Right of red line: Anomalies (error > {threshold:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Select Samples for Analysis\n",
    "\n",
    "We'll select:\n",
    "- **1 normal sample** (low error)\n",
    "- **1 anomaly sample** (high error)\n",
    "\n",
    "We'll explain these samples using SHAP and LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find normal and anomaly samples\n",
    "normal_indices = np.where(~is_anomaly)[0]\n",
    "anomaly_indices = np.where(is_anomaly)[0]\n",
    "\n",
    "# Select one of each\n",
    "normal_idx = normal_indices[100] # Pick 100th normal sample\n",
    "anomaly_idx = anomaly_indices[10] if len(anomaly_indices) > 10 else anomaly_indices[0]\n",
    "\n",
    "print(f\"Selected samples:\")\n",
    "print(f\" Normal sample: index {normal_idx}, error = {errors[normal_idx]:.4f}\")\n",
    "print(f\" Anomaly sample: index {anomaly_idx}, error = {errors[anomaly_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Prepare for SHAP and LIME\n",
    "\n",
    "Both SHAP and LIME need:\n",
    "1. **Background data** (subset of training data)\n",
    "2. **Prediction function** (model that takes X and returns error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample background data (100 random samples)\n",
    "background_size = 100\n",
    "background_indices = np.random.choice(len(X_test_normalized), size=background_size, replace=False)\n",
    "background_data = X_test_normalized[background_indices]\n",
    "\n",
    "print(f\"Background data shape: {background_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict_fn(X):\n",
    "\"\"\"Wrapper function that returns reconstruction errors.\"\"\"\n",
    "return exp_utils.calculate_reconstruction_error(model, X, device)\n",
    "\n",
    "# Test prediction function\n",
    "test_prediction = predict_fn(X_test_normalized[:5])\n",
    "print(f\"Test predictions: {test_prediction}\")\n",
    "print(\" Prediction function working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to explain (20 random samples)\n",
    "n_samples_to_explain = 20\n",
    "sample_indices = np.random.choice(len(X_test_normalized), size=n_samples_to_explain, replace=False)\n",
    "X_explain = X_test_normalized[sample_indices]\n",
    "\n",
    "print(f\"Sampled {n_samples_to_explain} instances to explain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: TODO 1 - Setup SHAP Explainer\n",
    "\n",
    "**Your task:** Complete the `create_shap_explainer()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `create_shap_explainer()` function (line ~40)\n",
    "3. Replace the `NotImplementedError` with:\n",
    "\n",
    "```python\n",
    "explainer = shap.KernelExplainer(predict_fn, background_data)\n",
    "return explainer\n",
    "```\n",
    "\n",
    "**Then run the cell below to test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Setup SHAP explainer\n",
    "try:\n",
    "shap_explainer = exp_utils.create_shap_explainer(predict_fn, background_data)\n",
    "print(\" TODO 1 COMPLETE: SHAP explainer created successfully!\")\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 1 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the create_shap_explainer() function in models/explainability_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: TODO 2 - Calculate SHAP Values\n",
    "\n",
    "**Your task:** Complete the `calculate_shap_values()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `calculate_shap_values()` function (line ~60)\n",
    "3. Replace the `NotImplementedError` with:\n",
    "\n",
    "```python\n",
    "shap_values = explainer.shap_values(X, nsamples=nsamples)\n",
    "return shap_values\n",
    "```\n",
    "\n",
    "**Note:** This may take 2-3 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Calculate SHAP values\n",
    "try:\n",
    "print(\"Calculating SHAP values... (this may take 2-3 minutes)\")\n",
    "shap_values = exp_utils.calculate_shap_values(shap_explainer, X_explain, nsamples=50)\n",
    "print(f\" TODO 2 COMPLETE: SHAP values calculated! Shape: {shap_values.shape}\")\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 2 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the calculate_shap_values() function in models/explainability_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Global SHAP Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global feature importance (mean absolute SHAP value)\n",
    "if 'shap_values' in locals():\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(mean_abs_shap)[::-1]\n",
    "sorted_features = [feature_names[i] for i in sorted_indices]\n",
    "sorted_importance = mean_abs_shap[sorted_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_features)), sorted_importance, color='steelblue')\n",
    "plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "plt.xlabel('Mean |SHAP value|', fontweight='bold', fontsize=12)\n",
    "plt.title('Global Feature Importance (SHAP)', fontweight='bold', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 most important sensors:\")\n",
    "for i in range(5):\n",
    "print(f\"{i+1}. {sorted_features[i]}: {sorted_importance[i]:.4f}\")\n",
    "else:\n",
    "print(\" Complete TODO 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Waterfall for Normal Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which explained sample corresponds to our normal sample\n",
    "if 'shap_values' in locals():\n",
    "# Find normal sample in explained samples\n",
    "normal_shap_idx = np.where(sample_indices == normal_idx)[0]\n",
    "\n",
    "if len(normal_shap_idx) > 0:\n",
    "shap_normal = shap_values[normal_shap_idx[0]]\n",
    "fig = exp_utils.plot_shap_waterfall(\n",
    "shap_normal,\n",
    "feature_names,\n",
    "title=f\"SHAP Waterfall - Normal Sample (error={errors[normal_idx]:.4f})\"\n",
    ")\n",
    "plt.show()\n",
    "else:\n",
    "print(\"Normal sample not in explained samples. Re-run Part 4 to include it.\")\n",
    "else:\n",
    "print(\" Complete TODO 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "**What does a positive SHAP value mean for sensor_4 in the context of anomaly detection?**\n",
    "\n",
    "*Write your answer here (double-click to edit):*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.2\n",
    "\n",
    "**What does a negative SHAP value mean?**\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: TODO 3 - Setup LIME Explainer\n",
    "\n",
    "**Your task:** Complete the `create_lime_explainer()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `create_lime_explainer()` function (line ~80)\n",
    "3. Replace the `NotImplementedError` with:\n",
    "\n",
    "```python\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "training_data=background_data,\n",
    "feature_names=feature_names,\n",
    "mode=mode,\n",
    "verbose=False\n",
    ")\n",
    "return explainer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Setup LIME explainer\n",
    "try:\n",
    "lime_explainer = exp_utils.create_lime_explainer(background_data, feature_names, mode='regression')\n",
    "print(\" TODO 3 COMPLETE: LIME explainer created successfully!\")\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 3 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the create_lime_explainer() function in models/explainability_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "\n",
    "**Why does LIME use perturbations? What is it trying to learn?**\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: TODO 4 - Generate LIME Explanations\n",
    "\n",
    "**Your task:** Complete the `explain_instance_lime()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `explain_instance_lime()` function (line ~100)\n",
    "3. Replace the `NotImplementedError` with:\n",
    "\n",
    "```python\n",
    "explanation = explainer.explain_instance(\n",
    "data_row=instance,\n",
    "predict_fn=predict_fn,\n",
    "num_features=num_features,\n",
    "num_samples=num_samples\n",
    ")\n",
    "feature_weights = dict(explanation.as_list())\n",
    "return explanation, feature_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Generate LIME explanation for normal sample\n",
    "try:\n",
    "print(\"Generating LIME explanation for normal sample...\")\n",
    "instance_normal = X_test_normalized[normal_idx]\n",
    "lime_exp_normal, lime_weights_normal = exp_utils.explain_instance_lime(\n",
    "lime_explainer,\n",
    "instance_normal,\n",
    "predict_fn,\n",
    "num_features=14,\n",
    "num_samples=500\n",
    ")\n",
    "print(f\" TODO 4 COMPLETE: LIME explanation generated! Found {len(lime_weights_normal)} features\")\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 4 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the explain_instance_lime() function in models/explainability_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize LIME Weights for Normal Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lime_weights_normal' in locals():\n",
    "fig = exp_utils.plot_lime_weights(\n",
    "lime_weights_normal,\n",
    "title=f\"LIME Feature Weights - Normal Sample (error={errors[normal_idx]:.4f})\"\n",
    ")\n",
    "plt.show()\n",
    "else:\n",
    "print(\" Complete TODO 4 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LIME explanation for anomaly sample\n",
    "if 'lime_explainer' in locals():\n",
    "print(\"Generating LIME explanation for anomaly sample...\")\n",
    "instance_anomaly = X_test_normalized[anomaly_idx]\n",
    "lime_exp_anomaly, lime_weights_anomaly = exp_utils.explain_instance_lime(\n",
    "lime_explainer,\n",
    "instance_anomaly,\n",
    "predict_fn,\n",
    "num_features=14,\n",
    "num_samples=500\n",
    ")\n",
    "print(\" LIME explanation for anomaly generated!\")\n",
    "\n",
    "# Plot\n",
    "fig = exp_utils.plot_lime_weights(\n",
    "lime_weights_anomaly,\n",
    "title=f\"LIME Feature Weights - Anomaly Sample (error={errors[anomaly_idx]:.4f})\"\n",
    ")\n",
    "plt.show()\n",
    "else:\n",
    "print(\" Complete TODO 3 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: TODO 5 - Compare SHAP vs LIME\n",
    "\n",
    "**Your task:** Complete the `compare_shap_lime()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `compare_shap_lime()` function (line ~130)\n",
    "3. Replace the `NotImplementedError` with the comparison logic\n",
    "\n",
    "**Hint:** Check sign agreement - features agree if both have same sign (both positive or both negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Compare SHAP vs LIME for normal sample\n",
    "if 'shap_normal' in locals() and 'lime_weights_normal' in locals():\n",
    "try:\n",
    "agreement_rate_normal, disagreeing_features_normal = exp_utils.compare_shap_lime(\n",
    "shap_normal,\n",
    "lime_weights_normal,\n",
    "feature_names\n",
    ")\n",
    "print(f\" TODO 5 COMPLETE: SHAP-LIME Agreement (Normal): {agreement_rate_normal:.1%}\")\n",
    "if len(disagreeing_features_normal) > 0:\n",
    "print(f\"Disagreeing features: {disagreeing_features_normal}\")\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 5 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the compare_shap_lime() function in models/explainability_utils.py\")\n",
    "else:\n",
    "print(\" Complete TODO 2 and TODO 4 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize SHAP vs LIME Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shap_normal' in locals() and 'lime_weights_normal' in locals():\n",
    "fig = exp_utils.plot_shap_vs_lime(\n",
    "shap_normal,\n",
    "lime_weights_normal,\n",
    "feature_names,\n",
    "title=\"SHAP vs LIME - Normal Sample\"\n",
    ")\n",
    "plt.show()\n",
    "else:\n",
    "print(\" Complete TODO 2 and TODO 4 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "\n",
    "**If SHAP and LIME disagree on a feature, which one should you trust more? Why?**\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.2\n",
    "\n",
    "**What does high agreement (>80%) tell you about the model's predictions?**\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: TODO 6 - Create Engineer-Friendly Interpretation\n",
    "\n",
    "**Your task:** Complete the `interpret_for_engineer()` function in `models/explainability_utils.py`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `models/explainability_utils.py`\n",
    "2. Find the `interpret_for_engineer()` function (line ~160)\n",
    "3. Create a clear, actionable report for maintenance engineers\n",
    "\n",
    "**Requirements:**\n",
    "- Identify top 3 most important sensors\n",
    "- Explain status (normal/anomaly)\n",
    "- Provide actionable recommendations\n",
    "- Use plain language (no technical jargon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Create engineer interpretation for normal sample\n",
    "if 'shap_normal' in locals():\n",
    "try:\n",
    "interpretation_normal = exp_utils.interpret_for_engineer(\n",
    "shap_normal,\n",
    "feature_names,\n",
    "errors[normal_idx],\n",
    "threshold,\n",
    "errors[normal_idx] > threshold,\n",
    "top_n=3\n",
    ")\n",
    "print(\" TODO 6 COMPLETE: Engineer interpretation created!\\n\")\n",
    "print(interpretation_normal)\n",
    "except NotImplementedError as e:\n",
    "print(f\" TODO 6 NOT COMPLETE: {e}\")\n",
    "print(\"Please complete the interpret_for_engineer() function in models/explainability_utils.py\")\n",
    "else:\n",
    "print(\" Complete TODO 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "\n",
    "**How would you explain to a maintenance engineer (non-technical) why sensor_11 is important for detecting degradation?**\n",
    "\n",
    "*Write your answer in plain language (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Summary Report\n",
    "\n",
    "Generate a comprehensive summary of your explainability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shap_values' in locals():\n",
    "exp_utils.print_summary_report(shap_values, feature_names, errors, threshold, is_anomaly)\n",
    "else:\n",
    "print(\" Complete TODO 2 first to see summary report!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "### What to Submit:\n",
    "\n",
    "1. **Code:**\n",
    "- Completed `models/explainability_utils.py` (all 6 TODOs filled)\n",
    "- This notebook with all cells executed\n",
    "\n",
    "2. **Answers:**\n",
    "- Q2.1: Positive SHAP value meaning?\n",
    "- Q2.2: Negative SHAP value meaning?\n",
    "- Q3.1: Why LIME uses perturbations?\n",
    "- Q4.1: SHAP vs LIME disagreement?\n",
    "- Q4.2: High agreement interpretation?\n",
    "- Q5.1: Explain sensor importance to engineer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations! \n",
    "\n",
    "You've completed Day 2 TP on Explainability!\n",
    "\n",
    "**You now have a production-ready ML pipeline:**\n",
    "- Day 1: Data exploration + Model training + MLflow tracking\n",
    "- Day 2: Explainability with SHAP + LIME\n",
    "\n",
    "**You learned:**\n",
    "- How to apply SHAP for global and local explanations\n",
    "- How to apply LIME for instance-level interpretability\n",
    "- How to compare different explainability methods\n",
    "- How to communicate ML results to non-technical stakeholders\n",
    "- MLOps best practices (modular code, reusable utilities)\n",
    "\n",
    "**Next steps:**\n",
    "- Apply these techniques to your own ML projects\n",
    "- Explore other XAI methods (Integrated Gradients, Attention, etc.)\n",
    "- Deploy your model with explainability API\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Contact: feda.almuhisen@univ-amu.fr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}